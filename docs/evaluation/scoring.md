# Scoring Details

How scores are calculated in the evaluation engine.

---

## Overview

The evaluation engine computes:

1. **Dimension scores** (0-10 for each of 5 dimensions)
2. **Overall score** (weighted average)
3. **Recommendations** (based on scores)

---

## Dimension Scoring

### Completion Score

```python
def score_completion(events, profile):
    # Count shipped deliverables and completed weeks
    shipped = count_events(events, "deliverable_shipped")
    completed = count_events(events, "week_complete")

    # Calculate expected based on current position
    expected_weeks = (current_month - 1) * 4 + current_week

    # Completion rate
    rate = completed / max(expected_weeks, 1)

    # Score 0-10 based on rate
    return min(int(rate * 10), 10)
```

### Quality Score

```python
def score_quality(events, profile):
    # Check week completion status
    weeks = get_events(events, "week_complete")

    complete_count = count_with_status(weeks, "complete")
    partial_count = count_with_status(weeks, "partial")

    # Ratio of complete vs. partial
    if weeks:
        complete_rate = complete_count / len(weeks)
        score = 5 + int(complete_rate * 5)  # 5-10 range
    else:
        score = 7  # Default

    return score
```

### Consistency Score

```python
def score_consistency(events, profile):
    # Get unique days with events in last 14 days
    recent = get_recent_events(events, days=14)
    unique_days = count_unique_dates(recent)

    # Expect activity on ~10 days (weekdays)
    expected = 10
    rate = unique_days / expected

    return min(int(rate * 10), 10)
```

### Depth Score

```python
def score_depth(events, profile):
    # Look for stretch and research indicators
    stretch = count_events_containing(events, "stretch")
    research = count_events(events, "research")

    depth_indicators = stretch + research

    if depth_indicators >= 5:
        return 10
    elif depth_indicators >= 3:
        return 8
    elif depth_indicators >= 1:
        return 6
    else:
        return 5  # Neutral
```

### Reflection Score

```python
def score_reflection(events, profile):
    # Count retros
    retros = count_events_containing(events, "retro")

    # Count best practices entries
    practices = count_best_practices()

    score = min(retros * 2 + practices, 10)
    return max(score, 5)  # Minimum 5
```

---

## Overall Score Calculation

```python
WEIGHTS = {
    "completion": 0.30,
    "quality": 0.25,
    "consistency": 0.20,
    "depth": 0.15,
    "reflection": 0.10,
}

def compute_overall(dimensions):
    total = 0.0
    for dim, score in dimensions.items():
        weight = WEIGHTS[dim]
        total += score * weight
    return round(total, 2)
```

**Example**:

| Dimension | Score | Weight | Weighted |
|-----------|-------|--------|----------|
| Completion | 7 | 0.30 | 2.1 |
| Quality | 8 | 0.25 | 2.0 |
| Consistency | 6 | 0.20 | 1.2 |
| Depth | 8 | 0.15 | 1.2 |
| Reflection | 9 | 0.10 | 0.9 |
| **Total** | | | **7.4** |

---

## Recommendation Generation

```python
def generate_recommendations(dimensions, overall):
    recommendations = []

    if dimensions["consistency"] < 7:
        recommendations.append(
            "Improve consistency by logging progress daily."
        )

    if dimensions["reflection"] < 7:
        recommendations.append(
            "Run weekly retros and capture best practices."
        )

    if dimensions["completion"] < 6:
        recommendations.append(
            "Focus on shipping. Consider /ship-mvp for scope."
        )

    if overall < 6.0:
        recommendations.append(
            "Consider a remediation week to consolidate."
        )

    return recommendations
```

---

## Source of Truth

**Important**: Memory files are the source of truth.

```
.claude/memory/*  ←── Source of truth (append-only)
       │
       ▼
paths/Advanced/tracker.md  ←── Derived artifact
```

The tracker can be regenerated by `report.py` based on memory.

---

## Score History

Scores are computed on-demand, not stored. To track history:

1. Run `/evaluate` regularly
2. Scores are shown to you
3. Optionally log to progress log

Example log entry:

```json
{
  "timestamp": "2026-01-26T17:00:00Z",
  "event": "evaluation_run",
  "overall_score": 7.4,
  "dimensions": {
    "completion": 7,
    "quality": 8,
    "consistency": 6,
    "depth": 8,
    "reflection": 9
  }
}
```

---

## Debugging Scores

### Unexpected Low Score

1. Run verbose evaluation:
   ```bash
   python .claude/path-engine/evaluate.py --verbose
   ```

2. Check which dimension is low

3. Review the signals for that dimension

4. Verify your progress log has expected entries

### Missing Events

If events aren't being counted:

```bash
# Check progress log
tail -20 .claude/memory/progress_log.jsonl

# Verify JSON is valid
python -c "import json; [json.loads(l) for l in open('.claude/memory/progress_log.jsonl')]"
```

### Score Seems Wrong

1. Verify memory files are current
2. Check event timestamps
3. Run `evaluate.py --verbose`
4. Compare expected vs. actual signals

---

## Customizing Scoring

To adjust scoring:

1. Edit `.claude/path-engine/evaluate.py`
2. Modify the scoring functions
3. Update weights if needed
4. Update documentation

Example: Increase quality weight:

```python
WEIGHTS = {
    "completion": 0.25,  # Reduced
    "quality": 0.30,     # Increased
    "consistency": 0.20,
    "depth": 0.15,
    "reflection": 0.10,
}
```

---

## Links

- [Rubric](rubric.md)
- [Signals](signals.md)
- [Adaptation rules](adaptation-rules.md)
- [evaluate.py](../../.claude/path-engine/evaluate.py)
