# Week 03 — 2026

**Date**: January 13-19, 2026
**Month**: 2 (Sequence Models)
**Focus**: PyTorch Foundations

---

## Goals

What I plan to accomplish this week:

- [x] Set up sequence-models project with proper structure
- [ ] Master PyTorch tensors, autograd, and DataLoader
- [ ] Build and train feedforward classifier as baseline
- [ ] Prepare for RNN implementation next week

---

## Daily Log

### Monday (2 hrs)
- [x] Set up sequence-models project structure
- [x] PyTorch environment setup & verification
- [x] Tensors tutorial — creation, operations, GPU

**Notes**: Project scaffold created with pyproject.toml, src/, tests/. PyTorch 2.9.1 installed with MPS (Apple Silicon GPU) support. All 3 initial tests pass. Tensors notebook created covering creation, attributes, operations, reshaping, MPS acceleration (2.4x speedup), and autograd preview.

### Tuesday (2 hrs)
- [x] Autograd deep dive — gradients, backward pass
- [x] Build simple feedforward network

**Notes**: Covered computation graphs, backward(), gradient accumulation, no_grad(), and built a complete training loop that learns y=2x+1. Built FeedforwardClassifier with nn.Module, trained on moons dataset (89.5% accuracy), created reusable module with 5 tests.

### Wednesday (2.5 hrs)
- [x] Dataset and DataLoader patterns
- [x] Load IMDB sentiment dataset
- [x] Write data preprocessing module

**Notes**: Learned TensorDataset, custom Dataset class, DataLoader batching. Downloaded IMDB (25k train, 25k test). Built data module with clean_text, build_vocab, IMDBDataset. 11 data tests passing, 19 total.

### Thursday (2 hrs)
- [x] Complete training loop implementation
- [x] Add validation loop

**Notes**: Built Trainer class with train_epoch, evaluate, checkpointing. Created EmbeddingClassifier (mean pooling). Trained on IMDB: 85.1% accuracy in 3 epochs (11s on MPS). 25 tests passing.

### Friday (2 hrs)
- [x] Write tests for data loading and model
- [x] Create Jupyter notebook documenting learnings

**Notes**: Tests already done (25 passing). Created 05_week1_summary.ipynb consolidating all learnings: tensors, autograd, nn.Module, DataLoader, training loop, IMDB results.

### Weekend (4.5 hrs)
- [ ] Build simple neural network classifier
- [ ] Train on IMDB with proper logging
- [ ] Weekly retro & tracker update
- [ ] Research RNN architecture for next week

**Notes**:

---

## Reflections

### What went well?

- PyTorch foundation notebook summary was beautiful
- Proud of learning with AI-driven approach
- Logging every detail helps with retention
- Step-by-step approach from basic to advanced is effective

### What was challenging?

- Struggled to practice independently
- Need more exercises to complete on my own

### What did I learn?

- PyTorch autograd and computation graphs
- no_grad() for inference mode
- squeeze/unsqueeze for dimension manipulation
- Project structure patterns (src/, tests/, notebooks/)
- Optimizer usage (Adam, zero_grad, step)

### What will I try differently next week?

- Add exercises at the end of each notebook section
- Include research homework assignments
- Add comprehension questions after sections

---

## Best Practices Discovered

- **Step-by-step notebooks**: Progress from basic concepts to advanced, with runnable examples at each step
- **AI-driven learning with logging**: Document everything to reinforce retention

---

## Blockers & How I Resolved Them

| Blocker | Resolution |
|---------|------------|
| Python 3.14 + datasets library incompatibility | Built custom data loading module instead |
| Jupyter notebook path issues | Use relative paths from notebooks/ directory |

---

## Hours Logged

| Day | Planned | Actual |
|-----|---------|--------|
| Mon | 2 | |
| Tue | 2 | |
| Wed | 2.5 | |
| Thu | 2 | |
| Fri | 2 | |
| Sat/Sun | 4.5 | |
| **Total** | 15 | |

---

## Deliverables Checklist

- [x] Project scaffold with pyproject.toml, tests/
- [x] Data loading module with IMDB dataset
- [x] Basic training loop (reusable for RNN/LSTM)
- [x] Simple feedforward classifier as baseline
- [x] Jupyter notebook with PyTorch fundamentals

---

## Success Criteria

- [x] Feedforward classifier trains successfully
- [x] Baseline accuracy on IMDB documented (85.1%)
- [x] Tests pass for data loading (25 total)
- [x] Training loop works with proper loss/accuracy logging

---

## Next Week Preview

- Week 2: RNN Implementation
- Build RNN architecture from scratch
- Sentiment classification task
- Debug gradient issues
