# Week 2 — 2026

**Date**: January 10-16, 2026
**Month**: 1 — NLP Foundations
**Focus**: Text Preprocessing with NLTK

---

## Goals

What I plan to accomplish this week:

- [x] Set up project structure with pyproject.toml
- [x] Implement tokenization (word + sentence level)
- [x] Add text normalization (stopwords, stemming, lemmatization)
- [x] Write tests for core functionality
- [x] Create initial README with usage examples

---

## Daily Log

### Day 1-2: Project Setup & Tokenization (5 hrs)
- [x] Create project structure with `pyproject.toml`
- [x] Set up NLTK and download required corpora
- [x] Implement basic tokenization (word, sentence)
- [x] Add configurable tokenization strategies
- [x] Write initial tests for tokenization

**Notes**: Setup and NLTK usage was valuable learning experience.

### Day 3-4: Text Normalization (5 hrs)
- [x] Implement stopword removal
- [x] Add stemming (Porter, Snowball)
- [x] Add lemmatization with NLTK WordNet
- [x] Create unified preprocessing pipeline class

**Notes**: Porter and Snowball algorithms were new. Lemmatization was a key learning point.

### Day 5-6: Testing & Polish (4 hrs)
- [x] Write comprehensive tests for all components
- [x] Handle edge cases (empty strings, unicode, special chars)
- [x] Add CLI or script entry point
- [x] Write initial README with usage examples

**Notes**: Also completed embeddings and classifiers (ahead of schedule).

### Day 7: Reflection & Prep (1 hr)
- [x] Run `/retro` for Week 1 reflection
- [x] Log progress to memory
- [x] Preview Week 2 (embeddings)

**Notes**: Completed retro. Will research Week 2 topics to prepare for implementation.

---

## Reflections

*Completed 2026-01-10*

### What went well?

- Setup and using NLTK is valuable to learn
- Observed and reviewed all code that Claude built
- Proud of understanding all the coded components
- Pipeline structure and ordered approach worked well
- Comprehensive testing strategy was perfect
- All functions have docstrings (good documentation practice)

### What was challenging?

- Coding myself is challenging; prefer to observe coding first
- Porter and Snowball stemming algorithms were new concepts
- Lemmatization was a missing knowledge point before this week

### What did I learn?

- How text preprocessing pipelines are structured
- Difference between stemming (rule-based) and lemmatization (dictionary-based)
- Importance of comprehensive testing (114 tests!)
- Word embeddings and similarity search concepts
- Text classification approaches (Naive Bayes, embedding-based)

### What will I try differently next week?

- Research next week's subjects beforehand to be ready for implementation
- Try to code smaller components myself with Claude's guidance

---

## Best Practices Discovered

*(Added to .claude/memory/best_practices.md)*

- Observe-then-implement learning style: Watch code being built, understand it, then try similar patterns
- Comprehensive testing: Every module should have tests covering edge cases
- Documentation: All functions should have docstrings explaining purpose and parameters

---

## Blockers & How I Resolved Them

| Blocker | Resolution |
|---------|------------|
| Gensim incompatible with Python 3.14 | Built custom WordEmbeddings class with numpy |
| New algorithms (Porter, Snowball) | Learned through implementation and testing |

---

## Hours Logged

| Day | Planned | Actual |
|-----|---------|--------|
| Day 1-2 | 5 | 5 |
| Day 3-4 | 5 | 5 |
| Day 5-6 | 4 | 6 |
| Day 7 | 1 | 1 |
| **Total** | 15 | 17 |

---

## Next Week Preview

- Load pre-trained Word2Vec embeddings
- Implement GloVe loading
- Add FastText support
- Build similarity search
