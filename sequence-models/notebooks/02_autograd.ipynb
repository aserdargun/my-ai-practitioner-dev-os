{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd — Automatic Differentiation\n",
    "\n",
    "**Month 2, Week 1** — Sequence Models\n",
    "\n",
    "Autograd is PyTorch's automatic differentiation engine. It's how neural networks learn.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Training a neural network requires:\n",
    "1. **Forward pass**: compute output from input\n",
    "2. **Loss**: measure how wrong the output is\n",
    "3. **Backward pass**: compute gradients (how to adjust weights)\n",
    "4. **Update**: adjust weights to reduce loss\n",
    "\n",
    "Autograd handles step 3 automatically — you don't need to derive gradients by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Computation Graph\n",
    "\n",
    "When you set `requires_grad=True`, PyTorch builds a graph of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([3.], requires_grad=True)\n",
      "requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor that tracks gradients\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"requires_grad: {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x² = 9.0\n",
      "z = 2x² + 3 = 21.0\n",
      "\n",
      "z.grad_fn: <AddBackward0 object at 0x108720f40>\n"
     ]
    }
   ],
   "source": [
    "# Operations create a computation graph\n",
    "y = x ** 2       # y = x²\n",
    "z = 2 * y + 3    # z = 2y + 3 = 2x² + 3\n",
    "\n",
    "print(f\"y = x² = {y.item()}\")\n",
    "print(f\"z = 2x² + 3 = {z.item()}\")\n",
    "print(f\"\\nz.grad_fn: {z.grad_fn}\")  # Shows the operation that created z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Computing Gradients with backward()\n",
    "\n",
    "The chain rule: if z = 2y + 3 and y = x², then dz/dx = dz/dy × dy/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 3.0\n",
      "z = 2x² + 3\n",
      "dz/dx = 4x = 12.0\n",
      "Expected at x=3: 4×3 = 12 ✓\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"z = 2x² + 3\")\n",
    "print(f\"dz/dx = 4x = {x.grad.item()}\")\n",
    "print(f\"Expected at x=3: 4×3 = 12 ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Gradients with Multiple Variables\n",
    "\n",
    "In neural networks, we have many weights. Each gets its own gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = w1² + w1×w2 = 10.0\n",
      "\n",
      "dz/dw1 = 2×w1 + w2 = 7.0\n",
      "Expected: 2×2 + 3 = 7 ✓\n",
      "\n",
      "dz/dw2 = w1 = 2.0\n",
      "Expected: 2 ✓\n"
     ]
    }
   ],
   "source": [
    "# Two \"weights\"\n",
    "w1 = torch.tensor([2.0], requires_grad=True)\n",
    "w2 = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Forward pass: z = w1² + w1*w2\n",
    "z = w1 ** 2 + w1 * w2\n",
    "print(f\"z = w1² + w1×w2 = {z.item()}\")\n",
    "\n",
    "# Backward pass\n",
    "z.backward()\n",
    "\n",
    "print(f\"\\ndz/dw1 = 2×w1 + w2 = {w1.grad.item()}\")\n",
    "print(f\"Expected: 2×2 + 3 = 7 ✓\")\n",
    "\n",
    "print(f\"\\ndz/dw2 = w1 = {w2.grad.item()}\")\n",
    "print(f\"Expected: 2 ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Gradient Accumulation (Important!)\n",
    "\n",
    "Gradients **accumulate** by default. You must zero them before each backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1st backward: x.grad = 4.0\n",
      "After 2nd backward: x.grad = 8.0  (accumulated!)\n",
      "After zeroing + backward: x.grad = 4.0  ✓\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# First backward\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f\"After 1st backward: x.grad = {x.grad.item()}\")\n",
    "\n",
    "# Second backward WITHOUT zeroing\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f\"After 2nd backward: x.grad = {x.grad.item()}  (accumulated!)\")\n",
    "\n",
    "# Correct way: zero gradients first\n",
    "x.grad.zero_()\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f\"After zeroing + backward: x.grad = {x.grad.item()}  ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight**: In training loops, always call `optimizer.zero_grad()` before `loss.backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. torch.no_grad() — Disable Tracking\n",
    "\n",
    "During inference (prediction), we don't need gradients. Disabling saves memory and speeds up computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With tracking: y.requires_grad = True\n",
      "In no_grad: y.requires_grad = False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# With gradient tracking\n",
    "y = x ** 2\n",
    "print(f\"With tracking: y.requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# Without gradient tracking (inference mode)\n",
    "with torch.no_grad():\n",
    "    y_no_grad = x ** 2\n",
    "    print(f\"In no_grad: y.requires_grad = {y_no_grad.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. A Simple Neural Network Example\n",
    "\n",
    "Let's see autograd in action with a tiny \"network\" that learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning y = 2x + 1\n",
      "Initial: w=0.00, b=0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data: learn y = 2x + 1\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "Y = torch.tensor([[3.0], [5.0], [7.0], [9.0]])  # y = 2x + 1\n",
    "\n",
    "# Initialize weights (random start)\n",
    "w = torch.tensor([[0.0]], requires_grad=True)  # slope\n",
    "b = torch.tensor([[0.0]], requires_grad=True)  # intercept\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(\"Learning y = 2x + 1\")\n",
    "print(f\"Initial: w={w.item():.2f}, b={b.item():.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0: loss=41.0000, w=3.500, b=1.200\n",
      "Epoch  20: loss=0.0041, w=2.052, b=0.849\n",
      "Epoch  40: loss=0.0012, w=2.028, b=0.918\n",
      "Epoch  60: loss=0.0004, w=2.015, b=0.955\n",
      "Epoch  80: loss=0.0001, w=2.008, b=0.976\n",
      "\n",
      "Final: w=2.005, b=0.986\n",
      "Target: w=2.000, b=1.000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    Y_pred = X @ w + b  # predictions\n",
    "    \n",
    "    # Loss (mean squared error)\n",
    "    loss = ((Y_pred - Y) ** 2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights (gradient descent)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # Zero gradients for next iteration\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: loss={loss.item():.4f}, w={w.item():.3f}, b={b.item():.3f}\")\n",
    "\n",
    "print(f\"\\nFinal: w={w.item():.3f}, b={b.item():.3f}\")\n",
    "print(f\"Target: w=2.000, b=1.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Vector/Matrix Gradients\n",
    "\n",
    "In real networks, weights are matrices. Autograd handles this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x shape: torch.Size([1, 2])\n",
      "Weight W shape: torch.Size([2, 3])\n",
      "Output y shape: torch.Size([1, 3])\n",
      "\n",
      "W.grad shape: torch.Size([2, 3])\n",
      "W.grad:\n",
      "tensor([[-0.3314, -0.3314, -0.3314],\n",
      "        [ 1.3739,  1.3739,  1.3739]])\n"
     ]
    }
   ],
   "source": [
    "# Weight matrix (2 inputs → 3 outputs)\n",
    "W = torch.randn(2, 3, requires_grad=True)\n",
    "x = torch.randn(1, 2)  # 1 sample, 2 features\n",
    "\n",
    "# Forward: y = xW\n",
    "y = x @ W\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "print(f\"Weight W shape: {W.shape}\")\n",
    "print(f\"Output y shape: {y.shape}\")\n",
    "\n",
    "# Sum output (need scalar for backward)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nW.grad shape: {W.grad.shape}\")\n",
    "print(f\"W.grad:\\n{W.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. detach() — Stop Gradient Flow\n",
    "\n",
    "Sometimes you want to use a tensor's value but not backprop through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.requires_grad: True\n",
      "y_detached.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# Detach creates a new tensor that doesn't track history\n",
    "y_detached = y.detach()\n",
    "\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")\n",
    "print(f\"y_detached.requires_grad: {y_detached.requires_grad}\")\n",
    "\n",
    "# Use case: target in loss calculation shouldn't get gradients\n",
    "# loss = (prediction - target.detach()) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Code | Purpose |\n",
    "|---------|------|----------|\n",
    "| Track gradients | `requires_grad=True` | Enable autograd for this tensor |\n",
    "| Compute gradients | `.backward()` | Backprop through the graph |\n",
    "| Access gradient | `.grad` | Get the computed gradient |\n",
    "| Zero gradients | `.grad.zero_()` | Reset before next backward |\n",
    "| Disable tracking | `with torch.no_grad():` | Inference mode, saves memory |\n",
    "| Stop gradient | `.detach()` | Break the computation graph |\n",
    "\n",
    "## Training Loop Pattern\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = loss_fn(predictions, Y)\n",
    "    \n",
    "    # 2. Backward pass\n",
    "    optimizer.zero_grad()  # Zero gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    \n",
    "    # 3. Update weights\n",
    "    optimizer.step()       # Apply gradients\n",
    "```\n",
    "\n",
    "## Next\n",
    "\n",
    "Build a feedforward neural network using `torch.nn` and `torch.optim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
