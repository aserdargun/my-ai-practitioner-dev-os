{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks with torch.nn\n",
    "\n",
    "**Month 2, Week 1** — Sequence Models\n",
    "\n",
    "Now we use PyTorch's high-level API to build proper neural networks.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. `nn.Module` — base class for all models\n",
    "2. `nn.Linear` — fully connected layers\n",
    "3. Activation functions (ReLU, Sigmoid)\n",
    "4. `torch.optim` — optimizers\n",
    "5. Complete training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. nn.Module — The Base Class\n",
    "\n",
    "Every PyTorch model inherits from `nn.Module`. You define:\n",
    "- `__init__`: create layers\n",
    "- `forward`: define computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)   # Linear transformation\n",
    "        x = self.relu(x)     # Non-linearity\n",
    "        x = self.layer2(x)   # Output layer\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleNet(input_size=2, hidden_size=8, output_size=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect parameters\n",
    "print(\"Model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Create Training Data\n",
    "\n",
    "Let's create a binary classification problem (two moons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate data\n",
    "X_np, y_np = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split train/test\n",
    "train_size = 800\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_np[:, 0], X_np[:, 1], c=y_np, cmap='coolwarm', alpha=0.7)\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Loss Function and Optimizer\n",
    "\n",
    "- **Loss function**: measures how wrong predictions are\n",
    "- **Optimizer**: updates weights to reduce loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh model\n",
    "model = SimpleNet(input_size=2, hidden_size=16, output_size=1)\n",
    "\n",
    "# Binary classification loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer (Adam is a good default)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(f\"Loss: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ===== TRAINING =====\n",
    "    model.train()  # Set to training mode\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # ===== EVALUATION =====\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(X_test)\n",
    "            predictions = (torch.sigmoid(y_test_pred) > 0.5).float()\n",
    "            accuracy = (predictions == y_test).float().mean()\n",
    "            test_accuracies.append(accuracy.item())\n",
    "            \n",
    "        print(f\"Epoch {epoch:3d}: loss = {loss.item():.4f}, test_acc = {accuracy.item():.3f}\")\n",
    "\n",
    "print(f\"\\nFinal test accuracy: {accuracy.item():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, num_epochs, 10), test_accuracies)\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create grid\n",
    "x_min, x_max = X_np[:, 0].min() - 0.5, X_np[:, 0].max() + 0.5\n",
    "y_min, y_max = X_np[:, 1].min() - 0.5, X_np[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Predict on grid\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Z = torch.sigmoid(model(grid)).numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, levels=50, cmap='coolwarm', alpha=0.8)\n",
    "plt.scatter(X_np[:, 0], X_np[:, 1], c=y_np, cmap='coolwarm', edgecolors='black')\n",
    "plt.title('Decision Boundary')\n",
    "plt.colorbar(label='P(class=1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Using nn.Sequential (Shortcut)\n",
    "\n",
    "For simple architectures, use `nn.Sequential` instead of writing a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent model using Sequential\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(2, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "print(model_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|----------|\n",
    "| `nn.Module` | Base class for models |\n",
    "| `nn.Linear(in, out)` | Fully connected layer |\n",
    "| `nn.ReLU()` | Activation function |\n",
    "| `nn.BCEWithLogitsLoss()` | Binary classification loss |\n",
    "| `optim.Adam(params, lr)` | Optimizer |\n",
    "| `model.train()` | Enable dropout/batchnorm training behavior |\n",
    "| `model.eval()` | Disable for inference |\n",
    "\n",
    "## Training Loop Template\n",
    "\n",
    "```python\n",
    "model = MyModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        # Forward\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "## Next\n",
    "\n",
    "Add DataLoader for batch processing, then apply to IMDB sentiment classification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
